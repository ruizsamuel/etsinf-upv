{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrón aplicado a iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lectura del corpus:** $\\;$ comprobamos también que las matrices de datos y etiquetas tienen las filas y columnas que toca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150, 1) \n",
      " [[5.1015625  3.5        1.40039062 0.19995117 0.        ]\n",
      " [4.8984375  3.         1.40039062 0.19995117 0.        ]\n",
      " [4.69921875 3.19921875 1.29980469 0.19995117 0.        ]\n",
      " [4.6015625  3.09960938 1.5        0.19995117 0.        ]\n",
      " [5.         3.59960938 1.40039062 0.19995117 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; from sklearn.datasets import load_iris\n",
    "iris = load_iris(); X = iris.data.astype(np.float16);\n",
    "y = iris.target.astype(np.uint).reshape(-1, 1);\n",
    "print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partición del corpus:** $\\;$ Creamos un split de iris con un $20\\%$ de datos para test y el resto para entrenamiento (training), barajando previamente los datos de acuerdo con una semilla dada para la generación de números aleatorios. Aquí, como en todo código que incluya aleatoriedad (que requiera generar números aleatorios), conviene fijar dicha semilla para poder reproducir experimentos con exactitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementación de Perceptrón:** $\\;$ devuelve pesos en notación homogénea, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$ también el número de errores e iteraciones ejecutadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(X, y, b=0.1, a=1.0, K=200):\n",
    "    N, D = X.shape; Y = np.unique(y); C = Y.size; W = np.zeros((1+D, C))\n",
    "    for k in range(1, K+1):\n",
    "        E = 0\n",
    "        for n in range(N):\n",
    "            xn = np.array([1, *X[n, :]])\n",
    "            cn = np.squeeze(np.where(Y==y[n]))\n",
    "            gn = W[:,cn].T @ xn; err = False\n",
    "            for c in np.arange(C):\n",
    "                if c != cn and W[:,c].T @ xn + b >= gn:\n",
    "                    W[:, c] = W[:, c] - a*xn; err = True\n",
    "            if err:\n",
    "                W[:, cn] = W[:, cn] + a*xn; E = E + 1\n",
    "        if E == 0:\n",
    "            break;\n",
    "    return W, E, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aprendizaje de un clasificador (lineal) con Perceptrón:** $\\;$ Perceptrón minimiza el número de errores de entrenamiento (con margen)\n",
    "$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{I}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{x}_n\\biggr)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de iteraciones ejecutadas:  200\n",
      "Número de errores de entrenamiento:  2\n",
      "Vectores de pesos de las clases (en columnas y en notación homogénea):\n",
      " [[  10.           85.         -142.        ]\n",
      " [ -49.421875    -68.19140625 -176.47265625]\n",
      " [  50.171875     -1.72460938 -181.06445312]\n",
      " [-189.91210938  -87.70507812   68.69726562]\n",
      " [ -86.40258789 -137.78149414  157.88415527]]\n"
     ]
    }
   ],
   "source": [
    "W, E, k = perceptron(X_train, y_train)\n",
    "print(\"Número de iteraciones ejecutadas: \", k)\n",
    "print(\"Número de errores de entrenamiento: \", E)\n",
    "print(\"Vectores de pesos de las clases (en columnas y en notación homogénea):\\n\", W);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cálculo de la tasa de error en test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de error en test: 16.7%\n"
     ]
    }
   ],
   "source": [
    "X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n",
    "y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n",
    "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "print(f\"Tasa de error en test: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ajuste del margen:** $\\;$ experimento para aprender un valor de $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3 1000\n",
      "0.01 5 1000\n",
      "0.1 3 1000\n",
      "10 6 1000\n",
      "100 6 1000\n"
     ]
    }
   ],
   "source": [
    "for b in (.0, .01, .1, 10, 100):\n",
    "    W, E, k = perceptron(X_train, y_train, b=b, K=1000)\n",
    "    print(b, E, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación de resultados:** $\\;$ los datos de entrenamiento no parecen linealmente separables; no está claro que un margen mayor que cero pueda mejorar resultados, sobre todo porque solo tenemos $30$ muestras de test; con margen nulo ya hemos visto que se obtiene un error (en test) del $16.7\\%$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
